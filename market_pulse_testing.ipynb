{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Reddit Incremental Scraper\n",
    "print(\"üß™ Testing Reddit Incremental Scraper...\")\n",
    "\n",
    "# Run a small test\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    # Run the incremental scraper with small limits\n",
    "    result = subprocess.run([\n",
    "        sys.executable, \"-m\", \"ingest.reddit_incremental\", \n",
    "        \"scrape\", \n",
    "        \"--max-threads\", \"1\", \n",
    "        \"--max-comments-per-thread\", \"2\"\n",
    "    ], capture_output=True, text=True, cwd=\".\")\n",
    "    \n",
    "    print(\"‚úÖ Scraper output:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"‚ö†Ô∏è Warnings/Errors:\")\n",
    "        print(result.stderr)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running scraper: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Pulse Component Testing Notebook\n",
    "\n",
    "This notebook allows you to test each component of the Market Pulse system separately with real data.\n",
    "\n",
    "## Components to Test:\n",
    "1. **Database Connection & Ticker Loading**\n",
    "2. **GDELT Data Ingestion**\n",
    "3. **Content Scraping**\n",
    "4. **Ticker Linking**\n",
    "5. **Sentiment Analysis**\n",
    "6. **Context Analysis**|\n",
    "7. **End-to-End Pipeline**\n",
    "\n",
    "## Setup\n",
    "Make sure you have:\n",
    "- Database running (PostgreSQL)\n",
    "- Environment variables set (.env file)\n",
    "- Tickers seeded in database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded from .env file\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"‚úÖ Environment variables loaded from .env file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports and setup complete\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime, UTC, timedelta\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('.')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Imports and setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Database Connection & Ticker Loading Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connected successfully\n",
      "üìä Total tickers in database: 58\n",
      "\n",
      "üìà Sample tickers:\n",
      "  - AAPL: Apple Inc (aliases: 4)\n",
      "  - MSFT: Microsoft Corporation (aliases: 4)\n",
      "  - GOOGL: Alphabet Inc (aliases: 6)\n",
      "  - AMZN: Amazon.com Inc (aliases: 4)\n",
      "  - TSLA: Tesla Inc (aliases: 4)\n",
      "  - META: Meta Platforms Inc (aliases: 5)\n",
      "  - NVDA: NVIDIA Corporation (aliases: 4)\n",
      "  - BRK.B: Berkshire Hathaway Inc (aliases: 5)\n",
      "  - JPM: JPMorgan Chase & Co (aliases: 4)\n",
      "  - V: Visa Inc (aliases: 4)\n",
      "\n",
      "üì∞ Total articles in database: 5\n"
     ]
    }
   ],
   "source": [
    "# Test database connection and ticker loading\n",
    "from app.db.session import SessionLocal\n",
    "from app.db.models import Ticker, Article, ArticleTicker\n",
    "from sqlalchemy import select, func\n",
    "\n",
    "def test_database_connection():\n",
    "    \"\"\"Test database connection and basic queries.\"\"\"\n",
    "    try:\n",
    "        db = SessionLocal()\n",
    "        \n",
    "        # Test basic connection\n",
    "        result = db.execute(select(func.count(Ticker.symbol)))\n",
    "        ticker_count = result.scalar()\n",
    "        print(f\"‚úÖ Database connected successfully\")\n",
    "        print(f\"üìä Total tickers in database: {ticker_count}\")\n",
    "        \n",
    "        # Load sample tickers\n",
    "        tickers = db.execute(select(Ticker).limit(10)).scalars().all()\n",
    "        print(f\"\\nüìà Sample tickers:\")\n",
    "        for ticker in tickers:\n",
    "            print(f\"  - {ticker.symbol}: {ticker.name} (aliases: {len(ticker.aliases)})\")\n",
    "        \n",
    "        # Check articles\n",
    "        article_count = db.execute(select(func.count(Article.id))).scalar()\n",
    "        print(f\"\\nüì∞ Total articles in database: {article_count}\")\n",
    "        \n",
    "        db.close()\n",
    "        return tickers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run test\n",
    "sample_tickers = test_database_connection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['article_ticker', 'ticker', 'article']\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect\n",
    "from app.db.session import engine  # make sure you have engine\n",
    "\n",
    "inspector = inspect(engine)\n",
    "print(inspector.get_table_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT * FROM article_ticker LIMIT 5\"))\n",
    "    for row in result:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB URL: postgresql+psycopg://postgres:***@localhost:5432/market_pulse\n",
      "Driver: postgresql\n"
     ]
    }
   ],
   "source": [
    "from app.db.session import engine\n",
    "\n",
    "print(\"DB URL:\", engine.url)\n",
    "print(\"Driver:\", engine.name)   # will say \"postgresql\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reddit Data Ingestion Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 12:50:20,359 - ingest.reddit_parser - INFO - Reddit API client initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ Testing Reddit ingestion from r/wallstreetbets (limit: 5 posts)\n",
      "‚úÖ Reddit credentials found\n",
      "   Client ID: Q7UvV4ZY...\n",
      "   User Agent: MarketPulse/1.0 by MarketPulseBot\n",
      "‚úÖ Reddit parser initialized\n",
      "\n",
      "üì° Fetching latest posts from r/wallstreetbets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 12:50:21,100 - ingest.reddit_parser - INFO - Fetched 5 posts from r/wallstreetbets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fetched 5 posts from r/wallstreetbets\n",
      "\n",
      "üìù Parsing posts into Article objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 12:50:21,359 - ingest.reddit_parser - INFO - Fetched 5 posts from r/wallstreetbets\n",
      "2025-09-24 12:50:21,361 - ingest.reddit_parser - WARNING - Error parsing post 1nowbom: 'reddit_id' is an invalid keyword argument for Article\n",
      "2025-09-24 12:50:21,362 - ingest.reddit_parser - WARNING - Error parsing post 1noondm: 'reddit_id' is an invalid keyword argument for Article\n",
      "2025-09-24 12:50:21,364 - ingest.reddit_parser - WARNING - Error parsing post 1noiaeb: 'reddit_id' is an invalid keyword argument for Article\n",
      "2025-09-24 12:50:21,365 - ingest.reddit_parser - WARNING - Error parsing post 1nor507: 'reddit_id' is an invalid keyword argument for Article\n",
      "2025-09-24 12:50:21,367 - ingest.reddit_parser - WARNING - Error parsing post 1nouc7x: 'reddit_id' is an invalid keyword argument for Article\n",
      "2025-09-24 12:50:21,368 - ingest.reddit_parser - INFO - Parsed 0 articles from r/wallstreetbets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  No articles parsed from r/wallstreetbets\n"
     ]
    }
   ],
   "source": [
    "# Test Reddit data ingestion with latest posts\n",
    "from ingest.reddit_parser import RedditParser\n",
    "from ingest.reddit import get_reddit_credentials\n",
    "import os\n",
    "\n",
    "def test_reddit_ingestion(subreddit: str = \"wallstreetbets\", limit: int = 5):\n",
    "    \"\"\"Test Reddit data ingestion from a specific subreddit.\"\"\"\n",
    "    print(f\"üî¥ Testing Reddit ingestion from r/{subreddit} (limit: {limit} posts)\")\n",
    "    \n",
    "    try:\n",
    "        # Check for Reddit credentials\n",
    "        try:\n",
    "            client_id, client_secret, user_agent = get_reddit_credentials()\n",
    "            print(f\"‚úÖ Reddit credentials found\")\n",
    "            print(f\"   Client ID: {client_id[:8]}...\")\n",
    "            print(f\"   User Agent: {user_agent}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"‚ùå Reddit credentials not configured: {e}\")\n",
    "            print(\"   Please set REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET in your .env file\")\n",
    "            return []\n",
    "        \n",
    "        # Initialize Reddit parser\n",
    "        reddit_parser = RedditParser()\n",
    "        reddit_parser.initialize_reddit(client_id, client_secret, user_agent)\n",
    "        print(f\"‚úÖ Reddit parser initialized\")\n",
    "        \n",
    "        # Fetch posts from subreddit\n",
    "        print(f\"\\nüì° Fetching latest posts from r/{subreddit}...\")\n",
    "        posts = reddit_parser.fetch_subreddit_posts(\n",
    "            subreddit_name=subreddit,\n",
    "            limit=limit,\n",
    "            time_filter=\"day\"\n",
    "        )\n",
    "        \n",
    "        if not posts:\n",
    "            print(f\"‚ö†Ô∏è  No posts found in r/{subreddit}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"‚úÖ Fetched {len(posts)} posts from r/{subreddit}\")\n",
    "        \n",
    "        # Parse posts into Article objects\n",
    "        print(f\"\\nüìù Parsing posts into Article objects...\")\n",
    "        articles = reddit_parser.parse_subreddit_posts(\n",
    "            subreddit_name=subreddit,\n",
    "            limit=limit,\n",
    "            time_filter=\"day\"\n",
    "        )\n",
    "        \n",
    "        if not articles:\n",
    "            print(f\"‚ö†Ô∏è  No articles parsed from r/{subreddit}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"‚úÖ Parsed {len(articles)} articles\")\n",
    "        \n",
    "        # Display sample posts\n",
    "        print(f\"\\nüìã Sample Reddit posts from r/{subreddit}:\")\n",
    "        for i, article in enumerate(articles[:3], 1):\n",
    "            print(f\"\\n{i}. {article.title}\")\n",
    "            print(f\"   Author: u/{article.author}\")\n",
    "            print(f\"   Upvotes: {article.upvotes}\")\n",
    "            print(f\"   Comments: {article.num_comments}\")\n",
    "            print(f\"   Published: {article.published_at}\")\n",
    "            print(f\"   Reddit URL: {article.reddit_url}\")\n",
    "            if article.text and len(article.text) > 100:\n",
    "                print(f\"   Content: {article.text[:100]}...\")\n",
    "            else:\n",
    "                print(f\"   Content: {article.text}\")\n",
    "        \n",
    "        # Test ticker linking on Reddit posts\n",
    "        if sample_tickers:\n",
    "            print(f\"\\nüîó Testing ticker linking on Reddit posts...\")\n",
    "            from ingest.linker import TickerLinker\n",
    "            \n",
    "            linker = TickerLinker(sample_tickers, max_scraping_workers=2)\n",
    "            linking_results = []\n",
    "            \n",
    "            for i, article in enumerate(articles[:3], 1):\n",
    "                print(f\"\\nüì∞ Testing post {i}: {article.title[:50]}...\")\n",
    "                \n",
    "                # Link article to tickers\n",
    "                ticker_links = linker.link_article(article, use_title_only=True)\n",
    "                \n",
    "                print(f\"   Found {len(ticker_links)} ticker matches:\")\n",
    "                for link in ticker_links:\n",
    "                    print(f\"   - {link.ticker}: {link.confidence:.2f} confidence\")\n",
    "                    print(f\"     Matched terms: {link.matched_terms}\")\n",
    "                    print(f\"     Reasoning: {link.reasoning}\")\n",
    "                \n",
    "                linking_results.append((article, ticker_links))\n",
    "            \n",
    "            # Summary\n",
    "            total_links = sum(len(links) for _, links in linking_results)\n",
    "            linked_posts = sum(1 for _, links in linking_results if links)\n",
    "            \n",
    "            print(f\"\\nüìä Reddit Ticker Linking Summary:\")\n",
    "            print(f\"   Posts processed: {len(articles[:3])}\")\n",
    "            print(f\"   Posts with ticker links: {linked_posts}\")\n",
    "            print(f\"   Total ticker links: {total_links}\")\n",
    "        \n",
    "        return articles\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit ingestion test failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run Reddit test\n",
    "reddit_articles = test_reddit_ingestion(subreddit=\"wallstreetbets\", limit=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 12:51:56,802 - ingest.reddit_parser - INFO - Reddit API client initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reddit parser initialized\n",
      "\n",
      "üì° Fetching latest posts from r/wallstreetbets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 12:51:57,604 - ingest.reddit_parser - INFO - Fetched 10 posts from r/wallstreetbets\n"
     ]
    }
   ],
   "source": [
    "# Initialize Reddit parser\n",
    "client_id, client_secret, user_agent = get_reddit_credentials()\n",
    "subreddit='wallstreetbets'\n",
    "limit=10\n",
    "reddit_parser = RedditParser()\n",
    "reddit_parser.initialize_reddit(client_id, client_secret, user_agent)\n",
    "print(f\"‚úÖ Reddit parser initialized\")\n",
    "\n",
    "# Fetch posts from subreddit\n",
    "print(f\"\\nüì° Fetching latest posts from r/{subreddit}...\")\n",
    "posts = reddit_parser.fetch_subreddit_posts(\n",
    "    subreddit_name=subreddit,\n",
    "    limit=limit,\n",
    "    time_filter=\"day\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòÖ\n",
      "Daddy Powell just kicked us in the balls ‚ÄúStocks are overvalued‚Äù\n",
      "I went to the mall and EVERY girls was dressed like a skank - LULU to the MOON\n",
      "JPow with SPY\n",
      "$OPENed my butthole\n",
      "10 Years ago my net worth was $0\n",
      "Loaded up my grandmas retirement savings onto RIVN stock here\n",
      "When you buy the dip...\n",
      "Daily Discussion Thread for September 23, 2025\n",
      "What Are Your Moves Tomorrow, September 24, 2025\n"
     ]
    }
   ],
   "source": [
    "for post in posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple subreddits and compare content\n",
    "def test_multiple_subreddits(subreddits: list = None, limit_per_subreddit: int = 3):\n",
    "    \"\"\"Test Reddit ingestion from multiple subreddits and compare content.\"\"\"\n",
    "    if subreddits is None:\n",
    "        subreddits = [\"wallstreetbets\", \"stocks\", \"investing\"]\n",
    "    \n",
    "    print(f\"üî¥ Testing multiple subreddits: {', '.join(f'r/{s}' for s in subreddits)}\")\n",
    "    print(f\"   Limit per subreddit: {limit_per_subreddit} posts\")\n",
    "    \n",
    "    try:\n",
    "        # Check credentials\n",
    "        try:\n",
    "            client_id, client_secret, user_agent = get_reddit_credentials()\n",
    "        except ValueError as e:\n",
    "            print(f\"‚ùå Reddit credentials not configured: {e}\")\n",
    "            return {}\n",
    "        \n",
    "        # Initialize parser\n",
    "        reddit_parser = RedditParser()\n",
    "        reddit_parser.initialize_reddit(client_id, client_secret, user_agent)\n",
    "        \n",
    "        all_articles = {}\n",
    "        \n",
    "        for subreddit in subreddits:\n",
    "            print(f\"\\nüì° Processing r/{subreddit}...\")\n",
    "            \n",
    "            try:\n",
    "                articles = reddit_parser.parse_subreddit_posts(\n",
    "                    subreddit_name=subreddit,\n",
    "                    limit=limit_per_subreddit,\n",
    "                    time_filter=\"day\"\n",
    "                )\n",
    "                \n",
    "                if articles:\n",
    "                    all_articles[subreddit] = articles\n",
    "                    print(f\"   ‚úÖ {len(articles)} posts from r/{subreddit}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  No posts from r/{subreddit}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error processing r/{subreddit}: {e}\")\n",
    "        \n",
    "        # Display comparison\n",
    "        if all_articles:\n",
    "            print(f\"\\nüìä Subreddit Comparison:\")\n",
    "            print(f\"{'Subreddit':<15} {'Posts':<8} {'Avg Upvotes':<12} {'Avg Comments':<12} {'Ticker Mentions'}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for subreddit, articles in all_articles.items():\n",
    "                avg_upvotes = sum(a.upvotes or 0 for a in articles) / len(articles)\n",
    "                avg_comments = sum(a.num_comments or 0 for a in articles) / len(articles)\n",
    "                \n",
    "                # Count ticker mentions in titles\n",
    "                ticker_mentions = 0\n",
    "                for article in articles:\n",
    "                    title_lower = article.title.lower()\n",
    "                    # Look for common ticker patterns\n",
    "                    if any(pattern in title_lower for pattern in ['$', 'stock', 'ticker', 'earnings', 'dividend']):\n",
    "                        ticker_mentions += 1\n",
    "                \n",
    "                print(f\"r/{subreddit:<12} {len(articles):<8} {avg_upvotes:<12.1f} {avg_comments:<12.1f} {ticker_mentions}\")\n",
    "            \n",
    "            # Show sample posts from each subreddit\n",
    "            print(f\"\\nüìã Sample Posts by Subreddit:\")\n",
    "            for subreddit, articles in all_articles.items():\n",
    "                print(f\"\\nüî¥ r/{subreddit}:\")\n",
    "                for i, article in enumerate(articles[:2], 1):\n",
    "                    print(f\"   {i}. {article.title[:60]}...\")\n",
    "                    print(f\"      ‚Üë{article.upvotes} üí¨{article.num_comments} by u/{article.author}\")\n",
    "        \n",
    "        return all_articles\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Multiple subreddit test failed: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Run multiple subreddit test\n",
    "multi_subreddit_results = test_multiple_subreddits(limit_per_subreddit=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete Reddit ingestion pipeline\n",
    "def test_reddit_pipeline(subreddit: str = \"wallstreetbets\", limit: int = 5):\n",
    "    \"\"\"Test the complete Reddit ingestion pipeline from fetching to database storage.\"\"\"\n",
    "    print(f\"üöÄ Testing complete Reddit pipeline for r/{subreddit}\")\n",
    "    \n",
    "    try:\n",
    "        # Check credentials\n",
    "        try:\n",
    "            client_id, client_secret, user_agent = get_reddit_credentials()\n",
    "        except ValueError as e:\n",
    "            print(f\"‚ùå Reddit credentials not configured: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Step 1: Fetch Reddit posts\n",
    "        print(f\"\\nüì° Step 1: Fetching Reddit posts from r/{subreddit}...\")\n",
    "        reddit_parser = RedditParser()\n",
    "        reddit_parser.initialize_reddit(client_id, client_secret, user_agent)\n",
    "        \n",
    "        articles = reddit_parser.parse_subreddit_posts(\n",
    "            subreddit_name=subreddit,\n",
    "            limit=limit,\n",
    "            time_filter=\"day\"\n",
    "        )\n",
    "        \n",
    "        if not articles:\n",
    "            print(f\"‚ùå No articles fetched from r/{subreddit}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"‚úÖ Fetched {len(articles)} articles from r/{subreddit}\")\n",
    "        \n",
    "        # Step 2: Load tickers\n",
    "        print(f\"\\nüìà Step 2: Loading tickers...\")\n",
    "        db = SessionLocal()\n",
    "        tickers = db.execute(select(Ticker)).scalars().all()\n",
    "        db.close()\n",
    "        \n",
    "        if not tickers:\n",
    "            print(f\"‚ùå No tickers available\")\n",
    "            return None\n",
    "        \n",
    "        # Step 3: Initialize linker\n",
    "        print(f\"\\nüîó Step 3: Initializing ticker linker...\")\n",
    "        linker = TickerLinker(tickers, max_scraping_workers=2)\n",
    "        \n",
    "        # Step 4: Link articles to tickers\n",
    "        print(f\"\\nüîó Step 4: Linking Reddit posts to tickers...\")\n",
    "        linked_results = linker.link_articles_to_db(articles)\n",
    "        \n",
    "        # Step 5: Analyze sentiment\n",
    "        print(f\"\\nüòä Step 5: Analyzing sentiment...\")\n",
    "        sentiment_service = get_sentiment_service()\n",
    "        \n",
    "        pipeline_results = []\n",
    "        \n",
    "        for article, article_tickers in linked_results:\n",
    "            # Analyze sentiment\n",
    "            text_for_sentiment = article.text or article.title\n",
    "            if text_for_sentiment:\n",
    "                try:\n",
    "                    sentiment_score, sentiment_label = sentiment_service.analyze_with_label(text_for_sentiment)\n",
    "                except:\n",
    "                    sentiment_score, sentiment_label = 0.0, \"Neutral\"\n",
    "            else:\n",
    "                sentiment_score, sentiment_label = 0.0, \"Neutral\"\n",
    "            \n",
    "            pipeline_results.append({\n",
    "                'article': article,\n",
    "                'article_tickers': article_tickers,\n",
    "                'sentiment_score': sentiment_score,\n",
    "                'sentiment_label': sentiment_label,\n",
    "                'num_tickers': len(article_tickers),\n",
    "                'subreddit': article.subreddit,\n",
    "                'upvotes': article.upvotes,\n",
    "                'num_comments': article.num_comments\n",
    "            })\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\nüìä Reddit Pipeline Results Summary:\")\n",
    "        print(f\"   Articles processed: {len(articles)}\")\n",
    "        print(f\"   Articles with ticker links: {sum(1 for r in pipeline_results if r['num_tickers'] > 0)}\")\n",
    "        print(f\"   Total ticker relationships: {sum(r['num_tickers'] for r in pipeline_results)}\")\n",
    "        \n",
    "        if pipeline_results:\n",
    "            sentiment_labels = [r['sentiment_label'] for r in pipeline_results]\n",
    "            print(f\"   Sentiment distribution:\")\n",
    "            print(f\"     Positive: {sentiment_labels.count('Positive')}\")\n",
    "            print(f\"     Neutral: {sentiment_labels.count('Neutral')}\")\n",
    "            print(f\"     Negative: {sentiment_labels.count('Negative')}\")\n",
    "            \n",
    "            # Show top posts by engagement\n",
    "            print(f\"\\nüî• Top Posts by Engagement:\")\n",
    "            sorted_results = sorted(pipeline_results, key=lambda x: x['upvotes'] or 0, reverse=True)\n",
    "            for i, result in enumerate(sorted_results[:3], 1):\n",
    "                article = result['article']\n",
    "                print(f\"   {i}. {article.title[:60]}...\")\n",
    "                print(f\"      ‚Üë{result['upvotes']} üí¨{result['num_comments']} | {result['sentiment_label']} | {result['num_tickers']} tickers\")\n",
    "        \n",
    "        return pipeline_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit pipeline test failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run Reddit pipeline test\n",
    "reddit_pipeline_results = test_reddit_pipeline(subreddit=\"wallstreetbets\", limit=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GDELT Data Ingestion Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating Reddit-specific visualizations\n",
      "‚ùå Reddit visualization creation failed: name 'reddit_pipeline_results' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Create Reddit-specific visualizations\n",
    "def create_reddit_visualizations():\n",
    "    \"\"\"Create visualizations specifically for Reddit data.\"\"\"\n",
    "    print(\"üìä Creating Reddit-specific visualizations\")\n",
    "    \n",
    "    try:\n",
    "        if not reddit_pipeline_results:\n",
    "            print(\"‚ö†Ô∏è  No Reddit pipeline results available for visualization\")\n",
    "            return None\n",
    "        \n",
    "        # Set up plotting style\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Reddit Data Analysis Results', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Sentiment Distribution for Reddit\n",
    "        ax1 = axes[0, 0]\n",
    "        sentiment_labels = [r['sentiment_label'] for r in reddit_pipeline_results]\n",
    "        sentiment_counts = pd.Series(sentiment_labels).value_counts()\n",
    "        \n",
    "        colors = ['green' if label == 'Positive' else 'gray' if label == 'Neutral' else 'red' \n",
    "                 for label in sentiment_counts.index]\n",
    "        \n",
    "        sentiment_counts.plot(kind='bar', ax=ax1, color=colors)\n",
    "        ax1.set_title('Reddit Sentiment Distribution')\n",
    "        ax1.set_xlabel('Sentiment')\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 2. Upvotes vs Comments Scatter\n",
    "        ax2 = axes[0, 1]\n",
    "        upvotes = [r['upvotes'] or 0 for r in reddit_pipeline_results]\n",
    "        comments = [r['num_comments'] or 0 for r in reddit_pipeline_results]\n",
    "        \n",
    "        ax2.scatter(upvotes, comments, alpha=0.7, color='red')\n",
    "        ax2.set_title('Reddit Engagement: Upvotes vs Comments')\n",
    "        ax2.set_xlabel('Upvotes')\n",
    "        ax2.set_ylabel('Comments')\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(upvotes) > 1:\n",
    "            z = np.polyfit(upvotes, comments, 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax2.plot(upvotes, p(upvotes), \"r--\", alpha=0.8)\n",
    "        \n",
    "        # 3. Ticker Links per Post\n",
    "        ax3 = axes[1, 0]\n",
    "        ticker_counts = [r['num_tickers'] for r in reddit_pipeline_results]\n",
    "        \n",
    "        if ticker_counts:\n",
    "            ax3.hist(ticker_counts, bins=max(1, max(ticker_counts)), alpha=0.7, color='red', edgecolor='black')\n",
    "            ax3.set_title('Ticker Links per Reddit Post')\n",
    "            ax3.set_xlabel('Number of Ticker Links')\n",
    "            ax3.set_ylabel('Number of Posts')\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'No ticker links found', \n",
    "                    ha='center', va='center', transform=ax3.transAxes)\n",
    "            ax3.set_title('Ticker Links per Reddit Post')\n",
    "        \n",
    "        # 4. Top Posts by Engagement\n",
    "        ax4 = axes[1, 1]\n",
    "        if reddit_pipeline_results:\n",
    "            # Sort by upvotes and take top 5\n",
    "            top_posts = sorted(reddit_pipeline_results, key=lambda x: x['upvotes'] or 0, reverse=True)[:5]\n",
    "            \n",
    "            post_titles = [r['article'].title[:30] + '...' if len(r['article'].title) > 30 else r['article'].title \n",
    "                          for r in top_posts]\n",
    "            upvote_counts = [r['upvotes'] or 0 for r in top_posts]\n",
    "            \n",
    "            y_pos = range(len(post_titles))\n",
    "            ax4.barh(y_pos, upvote_counts, alpha=0.7, color='red')\n",
    "            ax4.set_yticks(y_pos)\n",
    "            ax4.set_yticklabels(post_titles, fontsize=8)\n",
    "            ax4.set_title('Top 5 Posts by Upvotes')\n",
    "            ax4.set_xlabel('Upvotes')\n",
    "        else:\n",
    "            ax4.text(0.5, 0.5, 'No Reddit data available', \n",
    "                    ha='center', va='center', transform=ax4.transAxes)\n",
    "            ax4.set_title('Top Posts by Engagement')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create Reddit summary table\n",
    "        print(\"\\nüìã Reddit Analysis Summary:\")\n",
    "        reddit_summary_data = []\n",
    "        \n",
    "        # Engagement metrics\n",
    "        total_upvotes = sum(r['upvotes'] or 0 for r in reddit_pipeline_results)\n",
    "        total_comments = sum(r['num_comments'] or 0 for r in reddit_pipeline_results)\n",
    "        avg_upvotes = total_upvotes / len(reddit_pipeline_results) if reddit_pipeline_results else 0\n",
    "        avg_comments = total_comments / len(reddit_pipeline_results) if reddit_pipeline_results else 0\n",
    "        \n",
    "        reddit_summary_data.append({\n",
    "            'Metric': 'Total Posts',\n",
    "            'Value': len(reddit_pipeline_results)\n",
    "        })\n",
    "        reddit_summary_data.append({\n",
    "            'Metric': 'Total Upvotes',\n",
    "            'Value': f\"{total_upvotes:,}\"\n",
    "        })\n",
    "        reddit_summary_data.append({\n",
    "            'Metric': 'Total Comments',\n",
    "            'Value': f\"{total_comments:,}\"\n",
    "        })\n",
    "        reddit_summary_data.append({\n",
    "            'Metric': 'Avg Upvotes/Post',\n",
    "            'Value': f\"{avg_upvotes:.1f}\"\n",
    "        })\n",
    "        reddit_summary_data.append({\n",
    "            'Metric': 'Avg Comments/Post',\n",
    "            'Value': f\"{avg_comments:.1f}\"\n",
    "        })\n",
    "        \n",
    "        # Sentiment metrics\n",
    "        sentiment_labels = [r['sentiment_label'] for r in reddit_pipeline_results]\n",
    "        reddit_summary_data.append({\n",
    "            'Metric': 'Positive Sentiment',\n",
    "            'Value': f\"{sentiment_labels.count('Positive')} ({sentiment_labels.count('Positive')/len(sentiment_labels)*100:.1f}%)\"\n",
    "        })\n",
    "        reddit_summary_data.append({\n",
    "            'Metric': 'Negative Sentiment',\n",
    "            'Value': f\"{sentiment_labels.count('Negative')} ({sentiment_labels.count('Negative')/len(sentiment_labels)*100:.1f}%)\"\n",
    "        })\n",
    "        \n",
    "        # Ticker linking metrics\n",
    "        posts_with_tickers = sum(1 for r in reddit_pipeline_results if r['num_tickers'] > 0)\n",
    "        total_ticker_links = sum(r['num_tickers'] for r in reddit_pipeline_results)\n",
    "        \n",
    "        reddit_summary_data.append({\n",
    "            'Metric': 'Posts with Ticker Links',\n",
    "            'Value': f\"{posts_with_tickers} ({posts_with_tickers/len(reddit_pipeline_results)*100:.1f}%)\"\n",
    "        })\n",
    "        reddit_summary_data.append({\n",
    "            'Metric': 'Total Ticker Links',\n",
    "            'Value': total_ticker_links\n",
    "        })\n",
    "        \n",
    "        # Display summary table\n",
    "        reddit_summary_df = pd.DataFrame(reddit_summary_data)\n",
    "        display(HTML(reddit_summary_df.to_html(index=False, escape=False)))\n",
    "        \n",
    "        return reddit_summary_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit visualization creation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create Reddit visualizations\n",
    "reddit_summary_df = create_reddit_visualizations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GDELT data ingestion with small sample\n",
    "import httpx\n",
    "import zipfile\n",
    "import io\n",
    "from ingest.parser import parse_gdelt_export_csv\n",
    "from ingest.gdelt import get_gdelt_export_urls, fetch_gdelt_file\n",
    "\n",
    "def test_gdelt_ingestion(hours_back: int = 1, max_files: int = 2):\n",
    "    \"\"\"Test GDELT data ingestion with limited data.\"\"\"\n",
    "    print(f\"üîç Testing GDELT ingestion for last {hours_back} hours (max {max_files} files)\")\n",
    "    \n",
    "    try:\n",
    "        # Get GDELT URLs\n",
    "        urls = get_gdelt_export_urls(hours_back)[:max_files]\n",
    "        print(f\"üì° Found {len(urls)} GDELT files to process\")\n",
    "        \n",
    "        articles = []\n",
    "        \n",
    "        for i, url in enumerate(urls, 1):\n",
    "            print(f\"\\nüì• Processing file {i}/{len(urls)}: {url.split('/')[-1]}\")\n",
    "            \n",
    "            # Fetch file content\n",
    "            content = fetch_gdelt_file(url)\n",
    "            if not content:\n",
    "                print(f\"‚ö†Ô∏è  Failed to fetch {url}\")\n",
    "                continue\n",
    "            \n",
    "            # Parse articles\n",
    "            file_articles = parse_gdelt_export_csv(content)\n",
    "            print(f\"üì∞ Parsed {len(file_articles)} articles from this file\")\n",
    "            \n",
    "            articles.extend(file_articles)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Total articles parsed: {len(articles)}\")\n",
    "        \n",
    "        # Display sample articles\n",
    "        if articles:\n",
    "            print(f\"\\nüìã Sample articles:\")\n",
    "            for i, article in enumerate(articles[:3], 1):\n",
    "                print(f\"\\n{i}. {article.title}\")\n",
    "                print(f\"   URL: {article.url}\")\n",
    "                print(f\"   Published: {article.published_at}\")\n",
    "                print(f\"   Source: {article.source}\")\n",
    "        \n",
    "        return articles\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GDELT ingestion test failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run test with small sample\n",
    "sample_articles = test_gdelt_ingestion(hours_back=1, max_files=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Content Scraping Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test content scraping component\n",
    "from app.services.content_scraper import get_content_scraper\n",
    "\n",
    "def test_content_scraping(articles: List[Any], max_articles: int = 3):\n",
    "    \"\"\"Test content scraping on sample articles.\"\"\"\n",
    "    print(f\"üï∑Ô∏è  Testing content scraping on {min(len(articles), max_articles)} articles\")\n",
    "    \n",
    "    if not articles:\n",
    "        print(\"‚ö†Ô∏è  No articles available for scraping test\")\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        scraper = get_content_scraper()\n",
    "        scraper.max_workers = 2  # Limit workers for testing\n",
    "        \n",
    "        # Select articles with valid URLs\n",
    "        test_articles = [a for a in articles[:max_articles] if a.url and a.url.startswith('http')]\n",
    "        \n",
    "        if not test_articles:\n",
    "            print(\"‚ö†Ô∏è  No valid URLs found for scraping\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"\\nüîó Testing URLs:\")\n",
    "        for article in test_articles:\n",
    "            print(f\"  - {article.url}\")\n",
    "        \n",
    "        # Test single URL scraping\n",
    "        print(f\"\\nüìÑ Testing single URL scraping:\")\n",
    "        first_url = test_articles[0].url\n",
    "        content = scraper.scrape_article_content(first_url)\n",
    "        \n",
    "        if content:\n",
    "            print(f\"‚úÖ Successfully scraped {len(content)} characters\")\n",
    "            print(f\"üìù Content preview: {content[:200]}...\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to scrape content from {first_url}\")\n",
    "        \n",
    "        # Test multithreaded scraping\n",
    "        print(f\"\\nüöÄ Testing multithreaded scraping:\")\n",
    "        urls = [article.url for article in test_articles]\n",
    "        scraped_results = scraper.scrape_articles_multithreaded(urls)\n",
    "        \n",
    "        successful_scrapes = sum(1 for content in scraped_results.values() if content is not None)\n",
    "        print(f\"‚úÖ Successfully scraped {successful_scrapes}/{len(urls)} URLs\")\n",
    "        \n",
    "        # Display results\n",
    "        for url, content in scraped_results.items():\n",
    "            if content:\n",
    "                print(f\"\\nüìÑ {url}:\")\n",
    "                print(f\"   Length: {len(content)} characters\")\n",
    "                print(f\"   Preview: {content[:150]}...\")\n",
    "            else:\n",
    "                print(f\"\\n‚ùå {url}: Failed to scrape\")\n",
    "        \n",
    "        return scraped_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Content scraping test failed: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Run test\n",
    "scraped_content = test_content_scraping(sample_articles, max_articles=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ticker Linking Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ticker linking component\n",
    "from ingest.linker import TickerLinker\n",
    "from app.models.dto import TickerLinkDTO\n",
    "\n",
    "def test_ticker_linking(articles: List[Any], tickers: List[Any], max_articles: int = 3):\n",
    "    \"\"\"Test ticker linking on sample articles.\"\"\"\n",
    "    print(f\"üîó Testing ticker linking on {min(len(articles), max_articles)} articles\")\n",
    "    \n",
    "    if not articles or not tickers:\n",
    "        print(\"‚ö†Ô∏è  No articles or tickers available for linking test\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Initialize linker\n",
    "        linker = TickerLinker(tickers, max_scraping_workers=2)\n",
    "        print(f\"‚úÖ TickerLinker initialized with {len(tickers)} tickers\")\n",
    "        \n",
    "        # Test on sample articles\n",
    "        test_articles = articles[:max_articles]\n",
    "        linking_results = []\n",
    "        \n",
    "        for i, article in enumerate(test_articles, 1):\n",
    "            print(f\"\\nüì∞ Testing article {i}: {article.title[:50]}...\")\n",
    "            \n",
    "            # Link article to tickers\n",
    "            ticker_links = linker.link_article(article)\n",
    "            \n",
    "            print(f\"   Found {len(ticker_links)} ticker matches\")\n",
    "            \n",
    "            for link in ticker_links:\n",
    "                print(f\"   - {link.ticker}: {link.confidence:.2f} confidence\")\n",
    "                print(f\"     Matched terms: {link.matched_terms}\")\n",
    "                print(f\"     Reasoning: {link.reasoning}\")\n",
    "            \n",
    "            linking_results.append((article, ticker_links))\n",
    "        \n",
    "        # Summary\n",
    "        total_links = sum(len(links) for _, links in linking_results)\n",
    "        linked_articles = sum(1 for _, links in linking_results if links)\n",
    "        \n",
    "        print(f\"\\nüìä Linking Summary:\")\n",
    "        print(f\"   Articles processed: {len(test_articles)}\")\n",
    "        print(f\"   Articles with links: {linked_articles}\")\n",
    "        print(f\"   Total ticker links: {total_links}\")\n",
    "        \n",
    "        return linking_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ticker linking test failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run test\n",
    "linking_results = test_ticker_linking(sample_articles, sample_tickers, max_articles=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentiment analysis component\n",
    "from app.services.sentiment import get_sentiment_service\n",
    "\n",
    "def test_sentiment_analysis(articles: List[Any], scraped_content: Dict[str, str]):\n",
    "    \"\"\"Test sentiment analysis on sample articles.\"\"\"\n",
    "    print(f\"üòä Testing sentiment analysis on {len(articles)} articles\")\n",
    "    \n",
    "    if not articles:\n",
    "        print(\"‚ö†Ô∏è  No articles available for sentiment test\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        sentiment_service = get_sentiment_service()\n",
    "        print(\"‚úÖ SentimentService initialized\")\n",
    "        \n",
    "        sentiment_results = []\n",
    "        \n",
    "        for i, article in enumerate(articles, 1):\n",
    "            print(f\"\\nüì∞ Analyzing sentiment for article {i}: {article.title[:50]}...\")\n",
    "            \n",
    "            # Get text for analysis (prefer scraped content, fallback to title)\n",
    "            text_to_analyze = None\n",
    "            if article.url in scraped_content and scraped_content[article.url]:\n",
    "                text_to_analyze = scraped_content[article.url]\n",
    "                print(f\"   Using scraped content ({len(text_to_analyze)} chars)\")\n",
    "            elif article.title:\n",
    "                text_to_analyze = article.title\n",
    "                print(f\"   Using title only ({len(text_to_analyze)} chars)\")\n",
    "            \n",
    "            if not text_to_analyze:\n",
    "                print(f\"   ‚ö†Ô∏è  No text available for analysis\")\n",
    "                continue\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            try:\n",
    "                score, label = sentiment_service.analyze_with_label(text_to_analyze)\n",
    "                print(f\"   Sentiment: {label} (score: {score:.3f})\")\n",
    "                \n",
    "                sentiment_results.append({\n",
    "                    'article': article,\n",
    "                    'text_length': len(text_to_analyze),\n",
    "                    'sentiment_score': score,\n",
    "                    'sentiment_label': label,\n",
    "                    'text_preview': text_to_analyze[:100] + \"...\" if len(text_to_analyze) > 100 else text_to_analyze\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Sentiment analysis failed: {e}\")\n",
    "        \n",
    "        # Summary\n",
    "        if sentiment_results:\n",
    "            scores = [r['sentiment_score'] for r in sentiment_results]\n",
    "            labels = [r['sentiment_label'] for r in sentiment_results]\n",
    "            \n",
    "            print(f\"\\nüìä Sentiment Analysis Summary:\")\n",
    "            print(f\"   Articles analyzed: {len(sentiment_results)}\")\n",
    "            print(f\"   Average score: {sum(scores)/len(scores):.3f}\")\n",
    "            print(f\"   Positive: {labels.count('Positive')}\")\n",
    "            print(f\"   Neutral: {labels.count('Neutral')}\")\n",
    "            print(f\"   Negative: {labels.count('Negative')}\")\n",
    "        \n",
    "        return sentiment_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Sentiment analysis test failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run test\n",
    "sentiment_results = test_sentiment_analysis(sample_articles, scraped_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Context Analyzer Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test context analyzer component\n",
    "from app.services.context_analyzer import get_context_analyzer\n",
    "\n",
    "def test_context_analyzer():\n",
    "    \"\"\"Test context analyzer with sample ticker mentions.\"\"\"\n",
    "    print(\"üß† Testing context analyzer with sample scenarios\")\n",
    "    \n",
    "    try:\n",
    "        analyzer = get_context_analyzer()\n",
    "        print(\"‚úÖ ContextAnalyzer initialized\")\n",
    "        \n",
    "        # Test scenarios\n",
    "        test_scenarios = [\n",
    "            {\n",
    "                'ticker': 'AAPL',\n",
    "                'text': 'Apple Inc reported strong quarterly earnings with revenue growth of 15%. The company\\'s stock price rose significantly.',\n",
    "                'matched_terms': ['AAPL', 'Apple'],\n",
    "                'description': 'Positive financial context for Apple'\n",
    "            },\n",
    "            {\n",
    "                'ticker': 'V',\n",
    "                'text': 'I need to apply for a visa to travel to Europe next month. The visa application process is quite complex.',\n",
    "                'matched_terms': ['V'],\n",
    "                'description': 'Negative context - visa application, not Visa Inc'\n",
    "            },\n",
    "            {\n",
    "                'ticker': 'TSLA',\n",
    "                'text': 'Tesla stock surged after the company announced new electric vehicle models. Investors are bullish on TSLA.',\n",
    "                'matched_terms': ['TSLA', 'Tesla'],\n",
    "                'description': 'Positive financial context for Tesla'\n",
    "            },\n",
    "            {\n",
    "                'ticker': 'CAT',\n",
    "                'text': 'My cat is very playful and loves to chase toys around the house.',\n",
    "                'matched_terms': ['CAT'],\n",
    "                'description': 'Negative context - pet cat, not Caterpillar'\n",
    "            },\n",
    "            {\n",
    "                'ticker': 'MA',\n",
    "                'text': 'Mastercard Inc announced a new partnership with fintech companies. MA stock is performing well.',\n",
    "                'matched_terms': ['MA', 'Mastercard'],\n",
    "                'description': 'Positive financial context for Mastercard'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, scenario in enumerate(test_scenarios, 1):\n",
    "            print(f\"\\nüß™ Test {i}: {scenario['description']}\")\n",
    "            print(f\"   Ticker: {scenario['ticker']}\")\n",
    "            print(f\"   Text: {scenario['text'][:80]}...\")\n",
    "            \n",
    "            # Analyze context\n",
    "            confidence, reasoning = analyzer.analyze_ticker_relevance(\n",
    "                scenario['ticker'],\n",
    "                scenario['text'],\n",
    "                scenario['matched_terms']\n",
    "            )\n",
    "            \n",
    "            print(f\"   Confidence: {confidence:.3f}\")\n",
    "            print(f\"   Reasoning: {reasoning}\")\n",
    "            \n",
    "            # Determine if result is correct\n",
    "            expected_positive = 'Positive' in scenario['description']\n",
    "            is_positive = confidence >= 0.5\n",
    "            is_correct = expected_positive == is_positive\n",
    "            \n",
    "            print(f\"   Expected: {'Positive' if expected_positive else 'Negative'}\")\n",
    "            print(f\"   Result: {'‚úÖ Correct' if is_correct else '‚ùå Incorrect'}\")\n",
    "            \n",
    "            results.append({\n",
    "                'scenario': scenario,\n",
    "                'confidence': confidence,\n",
    "                'reasoning': reasoning,\n",
    "                'is_correct': is_correct\n",
    "            })\n",
    "        \n",
    "        # Summary\n",
    "        correct_count = sum(1 for r in results if r['is_correct'])\n",
    "        total_count = len(results)\n",
    "        \n",
    "        print(f\"\\nüìä Context Analysis Summary:\")\n",
    "        print(f\"   Tests run: {total_count}\")\n",
    "        print(f\"   Correct predictions: {correct_count}\")\n",
    "        print(f\"   Accuracy: {correct_count/total_count:.1%}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Context analyzer test failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run test\n",
    "context_results = test_context_analyzer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. End-to-End Pipeline Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete end-to-end pipeline\n",
    "def test_end_to_end_pipeline():\n",
    "    \"\"\"Test the complete pipeline from GDELT ingestion to database storage.\"\"\"\n",
    "    print(\"üöÄ Testing complete end-to-end pipeline\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Get fresh GDELT data\n",
    "        print(\"\\nüì° Step 1: Fetching fresh GDELT data...\")\n",
    "        fresh_articles = test_gdelt_ingestion(hours_back=1, max_files=1)\n",
    "        \n",
    "        if not fresh_articles:\n",
    "            print(\"‚ùå No fresh articles available for pipeline test\")\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Load tickers\n",
    "        print(\"\\nüìà Step 2: Loading tickers...\")\n",
    "        db = SessionLocal()\n",
    "        tickers = db.execute(select(Ticker)).scalars().all()\n",
    "        db.close()\n",
    "        \n",
    "        if not tickers:\n",
    "            print(\"‚ùå No tickers available for pipeline test\")\n",
    "            return None\n",
    "        \n",
    "        # Step 3: Initialize linker\n",
    "        print(\"\\nüîó Step 3: Initializing ticker linker...\")\n",
    "        linker = TickerLinker(tickers, max_scraping_workers=2)\n",
    "        \n",
    "        # Step 4: Link articles with multithreaded scraping\n",
    "        print(\"\\nüï∑Ô∏è  Step 4: Linking articles with content scraping...\")\n",
    "        linked_results = linker.link_articles_with_multithreaded_scraping(fresh_articles)\n",
    "        \n",
    "        # Step 5: Analyze sentiment for linked articles\n",
    "        print(\"\\nüòä Step 5: Analyzing sentiment...\")\n",
    "        sentiment_service = get_sentiment_service()\n",
    "        \n",
    "        pipeline_results = []\n",
    "        \n",
    "        for article, ticker_links in linked_results:\n",
    "            if not ticker_links:  # Skip articles with no ticker links\n",
    "                continue\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            text_for_sentiment = article.text or article.title\n",
    "            if text_for_sentiment:\n",
    "                try:\n",
    "                    sentiment_score, sentiment_label = sentiment_service.analyze_with_label(text_for_sentiment)\n",
    "                except:\n",
    "                    sentiment_score, sentiment_label = 0.0, \"Neutral\"\n",
    "            else:\n",
    "                sentiment_score, sentiment_label = 0.0, \"Neutral\"\n",
    "            \n",
    "            pipeline_results.append({\n",
    "                'article': article,\n",
    "                'ticker_links': ticker_links,\n",
    "                'sentiment_score': sentiment_score,\n",
    "                'sentiment_label': sentiment_label,\n",
    "                'num_tickers': len(ticker_links)\n",
    "            })\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\nüìä Pipeline Results Summary:\")\n",
    "        print(f\"   Articles processed: {len(fresh_articles)}\")\n",
    "        print(f\"   Articles with ticker links: {len(pipeline_results)}\")\n",
    "        print(f\"   Total ticker relationships: {sum(r['num_tickers'] for r in pipeline_results)}\")\n",
    "        \n",
    "        if pipeline_results:\n",
    "            sentiment_labels = [r['sentiment_label'] for r in pipeline_results]\n",
    "            print(f\"   Sentiment distribution:\")\n",
    "            print(f\"     Positive: {sentiment_labels.count('Positive')}\")\n",
    "            print(f\"     Neutral: {sentiment_labels.count('Neutral')}\")\n",
    "            print(f\"     Negative: {sentiment_labels.count('Negative')}\")\n",
    "        \n",
    "        return pipeline_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå End-to-end pipeline test failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run end-to-end test\n",
    "pipeline_results = test_end_to_end_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Visualization & Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations and analysis\n",
    "def create_visualizations():\n",
    "    \"\"\"Create visualizations for the test results.\"\"\"\n",
    "    print(\"üìä Creating visualizations and analysis\")\n",
    "    \n",
    "    try:\n",
    "        # Set up plotting style\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Market Pulse Component Testing Results', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Sentiment Distribution\n",
    "        if sentiment_results:\n",
    "            ax1 = axes[0, 0]\n",
    "            sentiment_labels = [r['sentiment_label'] for r in sentiment_results]\n",
    "            sentiment_counts = pd.Series(sentiment_labels).value_counts()\n",
    "            \n",
    "            colors = ['green' if label == 'Positive' else 'gray' if label == 'Neutral' else 'red' \n",
    "                     for label in sentiment_counts.index]\n",
    "            \n",
    "            sentiment_counts.plot(kind='bar', ax=ax1, color=colors)\n",
    "            ax1.set_title('Sentiment Distribution')\n",
    "            ax1.set_xlabel('Sentiment')\n",
    "            ax1.set_ylabel('Count')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            axes[0, 0].text(0.5, 0.5, 'No sentiment data available', \n",
    "                           ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "            axes[0, 0].set_title('Sentiment Distribution')\n",
    "        \n",
    "        # 2. Ticker Linking Confidence Distribution\n",
    "        if linking_results:\n",
    "            ax2 = axes[0, 1]\n",
    "            all_confidences = []\n",
    "            for _, ticker_links in linking_results:\n",
    "                for link in ticker_links:\n",
    "                    all_confidences.append(link.confidence)\n",
    "            \n",
    "            if all_confidences:\n",
    "                ax2.hist(all_confidences, bins=10, alpha=0.7, color='blue', edgecolor='black')\n",
    "                ax2.set_title('Ticker Linking Confidence Distribution')\n",
    "                ax2.set_xlabel('Confidence Score')\n",
    "                ax2.set_ylabel('Frequency')\n",
    "                ax2.axvline(x=0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "                ax2.legend()\n",
    "            else:\n",
    "                ax2.text(0.5, 0.5, 'No linking data available', \n",
    "                        ha='center', va='center', transform=ax2.transAxes)\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'No linking data available', \n",
    "                           ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "            axes[0, 1].set_title('Ticker Linking Confidence Distribution')\n",
    "        \n",
    "        # 3. Context Analyzer Accuracy\n",
    "        if context_results:\n",
    "            ax3 = axes[1, 0]\n",
    "            correct_count = sum(1 for r in context_results if r['is_correct'])\n",
    "            total_count = len(context_results)\n",
    "            incorrect_count = total_count - correct_count\n",
    "            \n",
    "            labels = ['Correct', 'Incorrect']\n",
    "            sizes = [correct_count, incorrect_count]\n",
    "            colors = ['lightgreen', 'lightcoral']\n",
    "            \n",
    "            ax3.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "            ax3.set_title(f'Context Analyzer Accuracy\\\\n({correct_count}/{total_count} correct)')\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'No context analysis data available', \n",
    "                           ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title('Context Analyzer Accuracy')\n",
    "        \n",
    "        # 4. Pipeline Results Summary\n",
    "        if pipeline_results:\n",
    "            ax4 = axes[1, 1]\n",
    "            ticker_counts = [r['num_tickers'] for r in pipeline_results]\n",
    "            \n",
    "            if ticker_counts:\n",
    "                ax4.hist(ticker_counts, bins=max(1, max(ticker_counts)), alpha=0.7, color='purple', edgecolor='black')\n",
    "                ax4.set_title('Tickers per Article Distribution')\n",
    "                ax4.set_xlabel('Number of Tickers')\n",
    "                ax4.set_ylabel('Number of Articles')\n",
    "            else:\n",
    "                ax4.text(0.5, 0.5, 'No pipeline data available', \n",
    "                        ha='center', va='center', transform=ax4.transAxes)\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'No pipeline data available', \n",
    "                           ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title('Tickers per Article Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create summary table\n",
    "        print(\"\\\\nüìã Test Results Summary Table:\")\n",
    "        summary_data = []\n",
    "        \n",
    "        # Database test\n",
    "        summary_data.append({\n",
    "            'Component': 'Database Connection',\n",
    "            'Status': '‚úÖ Pass' if sample_tickers else '‚ùå Fail',\n",
    "            'Details': f'{len(sample_tickers) if sample_tickers else 0} tickers loaded'\n",
    "        })\n",
    "        \n",
    "        # GDELT test\n",
    "        summary_data.append({\n",
    "            'Component': 'GDELT Ingestion',\n",
    "            'Status': '‚úÖ Pass' if sample_articles else '‚ùå Fail',\n",
    "            'Details': f'{len(sample_articles)} articles parsed'\n",
    "        })\n",
    "        \n",
    "        # Content scraping test\n",
    "        successful_scrapes = sum(1 for content in scraped_content.values() if content is not None)\n",
    "        summary_data.append({\n",
    "            'Component': 'Content Scraping',\n",
    "            'Status': '‚úÖ Pass' if successful_scrapes > 0 else '‚ùå Fail',\n",
    "            'Details': f'{successful_scrapes}/{len(scraped_content)} URLs scraped'\n",
    "        })\n",
    "        \n",
    "        # Ticker linking test\n",
    "        total_links = sum(len(links) for _, links in linking_results)\n",
    "        summary_data.append({\n",
    "            'Component': 'Ticker Linking',\n",
    "            'Status': '‚úÖ Pass' if total_links > 0 else '‚ùå Fail',\n",
    "            'Details': f'{total_links} ticker links found'\n",
    "        })\n",
    "        \n",
    "        # Sentiment analysis test\n",
    "        summary_data.append({\n",
    "            'Component': 'Sentiment Analysis',\n",
    "            'Status': '‚úÖ Pass' if sentiment_results else '‚ùå Fail',\n",
    "            'Details': f'{len(sentiment_results)} articles analyzed'\n",
    "        })\n",
    "        \n",
    "        # Context analyzer test\n",
    "        if context_results:\n",
    "            correct_count = sum(1 for r in context_results if r['is_correct'])\n",
    "            accuracy = correct_count / len(context_results)\n",
    "            summary_data.append({\n",
    "                'Component': 'Context Analyzer',\n",
    "                'Status': '‚úÖ Pass' if accuracy >= 0.6 else '‚ö†Ô∏è Partial',\n",
    "                'Details': f'{accuracy:.1%} accuracy ({correct_count}/{len(context_results)})'\n",
    "            })\n",
    "        else:\n",
    "            summary_data.append({\n",
    "                'Component': 'Context Analyzer',\n",
    "                'Status': '‚ùå Fail',\n",
    "                'Details': 'No test data'\n",
    "            })\n",
    "        \n",
    "        # Pipeline test\n",
    "        summary_data.append({\n",
    "            'Component': 'End-to-End Pipeline',\n",
    "            'Status': '‚úÖ Pass' if pipeline_results else '‚ùå Fail',\n",
    "            'Details': f'{len(pipeline_results) if pipeline_results else 0} articles processed'\n",
    "        })\n",
    "        \n",
    "        # Display summary table\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        display(HTML(summary_df.to_html(index=False, escape=False)))\n",
    "        \n",
    "        return summary_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Visualization creation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create visualizations\n",
    "summary_df = create_visualizations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Testing Functions\n",
    "\n",
    "You can use these functions to test specific components with your own data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom testing functions for your own data\n",
    "def test_custom_article(text: str, title: str = None, url: str = None):\n",
    "    \"\"\"Test a custom article through the complete pipeline.\"\"\"\n",
    "    print(f\"üß™ Testing custom article: {title or 'Untitled'}\")\n",
    "    \n",
    "    try:\n",
    "        # Create article object\n",
    "        from app.db.models import Article\n",
    "        article = Article(\n",
    "            source=\"custom\",\n",
    "            url=url or \"https://example.com\",\n",
    "            published_at=datetime.now(UTC),\n",
    "            title=title or \"Custom Test Article\",\n",
    "            text=text,\n",
    "            lang=\"en\"\n",
    "        )\n",
    "        \n",
    "        # Load tickers\n",
    "        db = SessionLocal()\n",
    "        tickers = db.execute(select(Ticker)).scalars().all()\n",
    "        db.close()\n",
    "        \n",
    "        if not tickers:\n",
    "            print(\"‚ùå No tickers available\")\n",
    "            return None\n",
    "        \n",
    "        # Initialize components\n",
    "        linker = TickerLinker(tickers, max_scraping_workers=1)\n",
    "        sentiment_service = get_sentiment_service()\n",
    "        \n",
    "        # Test ticker linking\n",
    "        print(\"\\\\nüîó Testing ticker linking...\")\n",
    "        ticker_links = linker.link_article(article)\n",
    "        print(f\"   Found {len(ticker_links)} ticker matches:\")\n",
    "        for link in ticker_links:\n",
    "            print(f\"   - {link.ticker}: {link.confidence:.2f} confidence\")\n",
    "            print(f\"     Matched terms: {link.matched_terms}\")\n",
    "            print(f\"     Reasoning: {link.reasoning}\")\n",
    "        \n",
    "        # Test sentiment analysis\n",
    "        print(\"\\\\nüòä Testing sentiment analysis...\")\n",
    "        sentiment_score, sentiment_label = sentiment_service.analyze_with_label(text)\n",
    "        print(f\"   Sentiment: {sentiment_label} (score: {sentiment_score:.3f})\")\n",
    "        \n",
    "        return {\n",
    "            'article': article,\n",
    "            'ticker_links': ticker_links,\n",
    "            'sentiment_score': sentiment_score,\n",
    "            'sentiment_label': sentiment_label\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Custom article test failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_custom_url(url: str):\n",
    "    \"\"\"Test a custom URL by scraping and analyzing it.\"\"\"\n",
    "    print(f\"üåê Testing custom URL: {url}\")\n",
    "    \n",
    "    try:\n",
    "        # Test content scraping\n",
    "        scraper = get_content_scraper()\n",
    "        content = scraper.scrape_article_content(url)\n",
    "        \n",
    "        if not content:\n",
    "            print(\"‚ùå Failed to scrape content from URL\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"‚úÖ Successfully scraped {len(content)} characters\")\n",
    "        \n",
    "        # Test with scraped content\n",
    "        return test_custom_article(\n",
    "            text=content,\n",
    "            title=f\"Article from {url}\",\n",
    "            url=url\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Custom URL test failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "print(\"\\\\nüìù Example usage:\")\n",
    "print(\"# Test a custom article:\")\n",
    "print(\"result = test_custom_article(\")\n",
    "print(\"    text='Apple Inc reported strong quarterly earnings. AAPL stock is up 5%.',\")\n",
    "print(\"    title='Apple Earnings Beat Expectations'\")\n",
    "print(\")\")\n",
    "print(\"\\\\n# Test a custom URL:\")\n",
    "print(\"result = test_custom_url('https://example-news-site.com/article')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion & Next Steps\n",
    "\n",
    "### What This Notebook Tests:\n",
    "\n",
    "1. **Database Connection** - Verifies PostgreSQL connection and ticker data\n",
    "2. **GDELT Ingestion** - Tests fetching and parsing GDELT data\n",
    "3. **Content Scraping** - Tests web scraping capabilities\n",
    "4. **Ticker Linking** - Tests article-to-ticker matching with confidence scores\n",
    "5. **Sentiment Analysis** - Tests VADER sentiment analysis\n",
    "6. **Context Analysis** - Tests ticker relevance determination\n",
    "7. **End-to-End Pipeline** - Tests complete workflow\n",
    "8. **Visualization** - Creates charts and summary tables\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "- PostgreSQL database running\n",
    "- Environment variables configured (.env file)\n",
    "- Tickers seeded in database (`uv run app/scripts/seed_tickers.py`)\n",
    "- Required Python packages installed\n",
    "\n",
    "### Usage Tips:\n",
    "\n",
    "1. **Run cells sequentially** - Each test builds on previous results\n",
    "2. **Adjust parameters** - Modify `hours_back`, `max_files`, `max_articles` for different data sizes\n",
    "3. **Use custom functions** - Test your own articles or URLs with the provided functions\n",
    "4. **Check logs** - Monitor console output for detailed component behavior\n",
    "5. **Review visualizations** - Use charts to understand system performance\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "- **Database errors**: Ensure PostgreSQL is running and .env is configured\n",
    "- **No GDELT data**: Try different time ranges or check GDELT server status\n",
    "- **Scraping failures**: Some URLs may be blocked or require different handling\n",
    "- **Import errors**: Ensure you're running from the project root directory\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Scale up testing** - Increase data volumes for performance testing\n",
    "2. **Add more tickers** - Expand ticker universe for better coverage\n",
    "3. **Fine-tune parameters** - Adjust confidence thresholds and scraping settings\n",
    "4. **Monitor in production** - Use this notebook for ongoing system health checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
