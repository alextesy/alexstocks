# Market Pulse Cron Jobs Example
# Add these to your crontab with: crontab -e

# ==============================================================================
# REDDIT SCRAPING JOBS (without sentiment analysis)
# ==============================================================================

# Scrape Reddit posts every 2 hours
0 */2 * * * cd /Users/alex/market-pulse-v2 && make reddit-ingest >> /var/log/market-pulse/reddit-posts.log 2>&1

# Scrape Reddit comments every 30 minutes (incremental)
*/30 * * * * cd /Users/alex/market-pulse-v2 && make reddit-incremental >> /var/log/market-pulse/reddit-comments.log 2>&1

# ==============================================================================
# SENTIMENT ANALYSIS JOBS (decoupled)
# ==============================================================================

# Analyze sentiment for all new articles every hour
0 * * * * cd /Users/alex/market-pulse-v2 && make analyze-sentiment-recent >> /var/log/market-pulse/sentiment.log 2>&1

# Analyze sentiment specifically for Reddit articles every 3 hours
0 */3 * * * cd /Users/alex/market-pulse-v2 && uv run python app/jobs/analyze_sentiment.py --source reddit --hours-back 3 --max-workers 6 >> /var/log/market-pulse/reddit-sentiment.log 2>&1

# ==============================================================================
# COMBINED JOBS (scraping + sentiment in sequence)
# ==============================================================================

# Combined job: Scrape Reddit posts and analyze sentiment every 4 hours
0 */4 * * * cd /Users/alex/market-pulse-v2 && make scrape-and-analyze-posts >> /var/log/market-pulse/combined-posts.log 2>&1

# Combined job: Scrape Reddit comments and analyze sentiment every hour
0 * * * * cd /Users/alex/market-pulse-v2 && make scrape-and-analyze-comments >> /var/log/market-pulse/combined-comments.log 2>&1

# ==============================================================================
# STOCK PRICE DATA COLLECTION
# ==============================================================================

# Collect current stock prices every 30 minutes during market hours (9:30 AM - 4:00 PM EST, Mon-Fri)
*/30 9-16 * * 1-5 cd /Users/alex/market-pulse-v2 && uv run python app/jobs/collect_stock_prices.py --type current >> /var/log/market-pulse/stock-prices.log 2>&1

# Collect current stock prices every 2 hours outside market hours
0 */2 17-23,0-8 * * * cd /Users/alex/market-pulse-v2 && uv run python app/jobs/collect_stock_prices.py --type current >> /var/log/market-pulse/stock-prices.log 2>&1

# Collect historical data once daily at 6 PM (after market close)
0 18 * * 1-5 cd /Users/alex/market-pulse-v2 && uv run python app/jobs/collect_stock_prices.py --type historical >> /var/log/market-pulse/stock-historical.log 2>&1

# ==============================================================================
# MAINTENANCE JOBS
# ==============================================================================

# Check Reddit scraping status daily at 9 AM
0 9 * * * cd /Users/alex/market-pulse-v2 && make reddit-status >> /var/log/market-pulse/status.log 2>&1

# ==============================================================================
# FLEXIBLE SCHEDULING EXAMPLES
# ==============================================================================

# Example 1: Peak trading hours - more frequent updates
# Monday-Friday 9:30 AM - 4:00 PM EST: Scrape every 15 minutes, sentiment every 30 minutes
*/15 9-16 * * 1-5 cd /Users/alex/market-pulse-v2 && make reddit-incremental >> /var/log/market-pulse/peak-scraping.log 2>&1
*/30 9-16 * * 1-5 cd /Users/alex/market-pulse-v2 && make analyze-sentiment-recent >> /var/log/market-pulse/peak-sentiment.log 2>&1

# Example 2: After-hours - less frequent updates
# Monday-Friday after 4 PM and before 9:30 AM: Scrape every 2 hours, sentiment every 4 hours
0 */2 17-23,0-8 * * 1-5 cd /Users/alex/market-pulse-v2 && make reddit-ingest >> /var/log/market-pulse/afterhours-scraping.log 2>&1
0 */4 17-23,0-8 * * 1-5 cd /Users/alex/market-pulse-v2 && make analyze-sentiment-reddit >> /var/log/market-pulse/afterhours-sentiment.log 2>&1

# Example 3: Weekend mode - basic monitoring
# Saturday-Sunday: Light scraping every 4 hours, sentiment every 6 hours
0 */4 * * 6,0 cd /Users/alex/market-pulse-v2 && make reddit-incremental >> /var/log/market-pulse/weekend-scraping.log 2>&1
0 */6 * * 6,0 cd /Users/alex/market-pulse-v2 && make analyze-sentiment-reddit >> /var/log/market-pulse/weekend-sentiment.log 2>&1

# ==============================================================================
# ADVANCED CONFIGURATIONS
# ==============================================================================

# Custom job: High-performance sentiment analysis with 8 workers
0 */2 * * * cd /Users/alex/market-pulse-v2 && uv run python app/jobs/analyze_sentiment.py --source reddit --hours-back 2 --max-workers 8 --batch-size 200 >> /var/log/market-pulse/fast-sentiment.log 2>&1

# Custom job: Process only r/wallstreetbets comments with immediate sentiment analysis
*/15 * * * * cd /Users/alex/market-pulse-v2 && uv run python app/jobs/scrape_and_analyze.py comments --subreddit wallstreetbets --max-threads 5 --sentiment-workers 4 >> /var/log/market-pulse/wsb-pipeline.log 2>&1

# ==============================================================================
# LOG ROTATION (optional - add to /etc/logrotate.d/market-pulse)
# ==============================================================================
# /var/log/market-pulse/*.log {
#     daily
#     rotate 7
#     compress
#     delaycompress
#     missingok
#     notifempty
#     copytruncate
# }
